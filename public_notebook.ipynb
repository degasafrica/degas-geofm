{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, tune, and deploy a custom ML model using Degas 100M from AWS Marketplace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Degas 100M: A geospatial foundational model that can be fine-tuned to your specific earth observation tasks. \n",
    "\n",
    "See the models' technical details here: https://arxiv.org/pdf/2405.02512v1\n",
    "\n",
    "This sample notebook shows you how to train a custom ML model using [Degas 100M](https://aws.amazon.com/marketplace/pp/prodview-6bl3fwy2k5h5i) from AWS Marketplace.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "## Pre-requisites\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. Some hands-on experience using [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "1. To use this algorithm successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to [Degas 100M](https://aws.amazon.com/marketplace/pp/prodview-6bl3fwy2k5h5i). \n",
    "\n",
    "## Contents\n",
    "1. [Subscribe to the algorithm](#1.-Subscribe-to-the-algorithm)\n",
    "1. [Prepare dataset](#2.-Prepare-dataset)\n",
    "\t1. [Dataset format expected by the algorithm](#A.-Dataset-format-expected-by-the-algorithm)\n",
    "\t1. [Configure and visualize train and test dataset](#B.-Configure-and-visualize-train-and-test-dataset)\n",
    "\t1. [Upload datasets to Amazon S3](#C.-Upload-datasets-to-Amazon-S3)\n",
    "1. [Train a machine learning model](#3:-Train-a-machine-learning-model)\n",
    "\t1. [Set up environment](#3.1-Set-up-environment)\n",
    "\t1. [Train a model](#3.2-Train-a-model)\n",
    "1. [Deploy model and verify results](#4:-Deploy-model-and-verify-results)\n",
    "    1. [Deploy trained model](#A.-Deploy-trained-model)\n",
    "    1. [Create input payload](#B.-Create-input-payload)\n",
    "    1. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "    1. [Visualize output](#D.-Visualize-output)\n",
    "    1. [Calculate relevant metrics](#E.-Calculate-relevant-metrics)\n",
    "    1. [Delete the endpoint](#F.-Delete-the-endpoint)\n",
    "1. [Perform Batch inference](#6.-Perform-Batch-inference)\n",
    "1. [Clean-up](#7.-Clean-up)\n",
    "\t1. [Delete the model](#A.-Delete-the-model)\n",
    "\t1. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "\n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the algorithm:\n",
    "1. Open the algorithm listing page [Degas 100M](https://aws.amazon.com/marketplace/pp/prodview-6bl3fwy2k5h5i).\n",
    "1. On the AWS Marketplace listing,  click on **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you agree with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn**. This is the algorithm ARN that you need to specify while training a custom ML model. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_arn = \"<Customer to specify algorithm ARN corresponding to their AWS region>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Dataset format expected by the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supported Types for training: tiff\n",
    "\n",
    "The user will need to:\n",
    "\n",
    "- Fill a template yaml file to configure the finetuning training task.\n",
    "\n",
    "- Provide a directory with the correct folder structure for the data splits.\n",
    "\n",
    "- Ensure the input files have the correct shape.\n",
    "\n",
    "Below more details on these requirements.\n",
    "\n",
    "Yaml configuration file:\n",
    "The yaml configuration file provides with task specific information to be used for finetuning the model. As such, it requires the user to provide dataset information, such as a path to the root folder or the name of the splits, and task-specific information such as the name of the satellite, the bands to use or the number of classes to predict.\n",
    "\n",
    "Directory structure:\n",
    "The datasets should be saved in a parent folder that contains at least two subfolders; one for training data, one for validation data. Additionally you can also add a test subfolder. \n",
    "\n",
    "For our input datasets, we adopt the same data format as used by the foundational model [Prithvi](https://arxiv.org/abs/2310.18660). In this format, data should be arranged into a collection of tiff files and their corresponding masks. Tiff files should be bigger than 224x224 pixel scenes where the first dimension represents the number of bands, and the other two are the spatial dimensions. The mask is used to associate each pixel with a label. The number of mask channels is equal to the number of classes in the scene, plus an extra band for missing data. For finetuning, your tiff will be center croped and converted to a 224x224 scene.\n",
    "\n",
    "#### Example input:\n",
    "Here you can see example datasets: [burn scars](https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars) and [multi temporal crop classification](https://huggingface.co/datasets/ibm-nasa-geospatial/multi-temporal-crop-classification).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find more information about dataset format in **Usage Information** section of  [Degas 100M](https://aws.amazon.com/marketplace/pp/prodview-6bl3fwy2k5h5i) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Configure and visualize train and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this example, we use the [burn scars](https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars) dataset. Please install tools to download and preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "!pip install rasterio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we download the dataset to a local directory and extract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "training_dataset = \"./dataset\"  \n",
    "snapshot_download(repo_id=\"ibm-nasa-geospatial/hls_burn_scars\", repo_type=\"dataset\", local_dir=training_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd {training_dataset} && tar -zxvf hls_burn_scars.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Upload datasets to Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We upload the dataset to S3 that will be used during SageMaker trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = sagemaker_session.upload_data(\n",
    "    training_dataset, bucket=bucket, key_prefix=\"degas-100M-burn-scar-dataset/training\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Train a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that dataset is available in an accessible Amazon S3 bucket, we are ready to train a machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "output_location = \"s3://{}/degas_100M_example/{}\".format(\n",
    "    bucket, \"output\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The burn-scar dataset is created based on the HLS, and its labels have two classes (Unburnt land and Burn scar), and training and validation splits are provided for training. We set hyperparameters to customize finetuning specifically for the task as shown below. You can also find more information about dataset format in **Hyperparameters** section of [Degas 100M](https://aws.amazon.com/marketplace/pp/prodview-6bl3fwy2k5h5i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "    \"training_args\": {\n",
    "        \"train_split\": \"training\",\n",
    "        \"val_split\": \"validation\",\n",
    "        \"test_split\": \"validation\",\n",
    "        \"lr\": 1.3e-05,\n",
    "        \"max_train_iters\": 6000,  # iterations for training\n",
    "        \"max_eval_iters\": 1000,  # evaluation interval       \n",
    "    },\n",
    "    \"task_args\": {  \n",
    "        \"satellite\": \"S2\",\n",
    "        \"bands\": [0, 1, 2, 3, 4, 5],\n",
    "        \"classes\": [\"Unburnt land\", \"Burn scar\"],\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information on creating an `Estimator` object, see [documentation](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "are_hp_in_yaml = False\n",
    "\n",
    "# Load yaml parameters if needed\n",
    "if are_hp_in_yaml:\n",
    "    params_path = \"./configs/params/fm_finetune_params.yaml\"\n",
    "    with open(params_path) as f:\n",
    "        hyperparameters = yaml.safe_load(f)\n",
    "\n",
    "estimator = sagemaker.algorithm.AlgorithmEstimator(\n",
    "    algorithm_arn=algo_arn,\n",
    "    base_job_name=\"degas-100M-finetune-burnscars\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    input_mode=\"File\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\n",
    "        \"training_args\": '\"' + str(yaml.dump(hyperparameters[\"training_args\"])).replace(\"\\n\", \"\\\\n\") + '\"',\n",
    "        \"task_args\": '\"' + str(yaml.dump(hyperparameters[\"task_args\"])).replace(\"\\n\", \"\\\\n\") + '\"'\n",
    "    },\n",
    ")\n",
    "\n",
    "# Run the training job.\n",
    "estimator.fit({\"training\": training_data})        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See this [blog-post](https://aws.amazon.com/blogs/machine-learning/easily-monitor-and-visualize-metrics-while-training-models-on-amazon-sagemaker/) for more information how to visualize metrics during the process. You can also open the training job from [Amazon SageMaker console](https://console.aws.amazon.com/sagemaker/home?#/jobs/) and monitor the metrics/logs in **Monitor** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Deploy model and verify results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can deploy the model for performing real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"degas-100m-finetuned-burn-scar\"\n",
    "\n",
    "content_type = \"application/x-npy\"\n",
    "\n",
    "real_time_inference_instance_type = (\n",
    "    \"ml.p3.2xlarge\"\n",
    ")\n",
    "batch_transform_inference_instance_type = (\n",
    "    \"ml.p3.2xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Deploy trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.base_serializers import NumpySerializer\n",
    "from sagemaker.base_deserializers import NumpyDeserializer\n",
    "\n",
    "predictor = estimator.deploy(\n",
    "    1, real_time_inference_instance_type, serializer=NumpySerializer(), deserializer=NumpyDeserializer(allow_pickle=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint is created, you can perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create input payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inference, converting a tif format file to a numpy array is required. \n",
    "The following cell performs the preprocessing with such as normalizing and resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio, io\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "\n",
    "sample = training_dataset + \"/validation/subsetted_512x512_HLS.S30.T10SEH.2018190.v1.4_merged.tif\"\n",
    "\n",
    "# Use rasterio to read the geospatial .tif file\n",
    "with rasterio.open(sample) as src:\n",
    "    # Read the entire image\n",
    "    image_array = src.read()\n",
    "\n",
    "# Optional: Process the image_array as needed for your model\n",
    "# For example, normalize, resize, etc.\n",
    "# Assuming the model expects a specific format\n",
    "\n",
    "img_norm_cfg = dict(\n",
    "    means=[\n",
    "        0.033349706741586264,\n",
    "        0.05701185520536176,\n",
    "        0.05889748132001316,\n",
    "        0.2323245113436119,\n",
    "        0.1972854853760658,\n",
    "        0.11944914225186566,\n",
    "    ],\n",
    "    stds=[\n",
    "        0.02269135568823774,\n",
    "        0.026807560223070237,\n",
    "        0.04004109844362779,\n",
    "        0.07791732423672691,\n",
    "        0.08708738838140137,\n",
    "        0.07241979477437814,\n",
    "    ],\n",
    ")\n",
    "\n",
    "image_array = image_array.astype(np.float32)\n",
    "image_array = np.where(image_array == None, 0.0, image_array)\n",
    "image_array = np.expand_dims(image_array, 1)\n",
    "\n",
    "# Define the desired shape\n",
    "target_shape = (6, 1, 224, 224)\n",
    "x_bias = 200\n",
    "y_bias = 0\n",
    "norm = F.normalize(\n",
    "    torch.from_numpy(image_array[:, 0, x_bias:target_shape[2]+x_bias, y_bias:target_shape[3]+y_bias]),\n",
    "    img_norm_cfg[\"means\"],\n",
    "    img_norm_cfg[\"stds\"],\n",
    "    False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the input sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "imgplot = plt.imshow(image_array[0, 0, x_bias:target_shape[2]+x_bias, y_bias:target_shape[3]+y_bias])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = predictor.predict(np.expand_dims(np.expand_dims(norm, 0), 2))\n",
    "pred_value, pred_label = torch.from_numpy(res)[0].topk(1, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(pred_label.detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Calculate relevant metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we simple calculate accuracy of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_label = training_dataset + \"/validation/subsetted_512x512_HLS.S30.T10SEH.2018190.v1.4.mask.tif\"\n",
    "\n",
    "with rasterio.open(sample_label) as src:\n",
    "    label_array = src.read()\n",
    "\n",
    "target = label_array[0, x_bias:target_shape[2]+x_bias, y_bias:target_shape[3]+y_bias]\n",
    "pred = pred_label[0].numpy()\n",
    "    \n",
    "acc = np.sum(np.equal(target, pred))/pred.size\n",
    "print(f\"{acc*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If [Amazon SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html) supports the type of problem you are trying to solve using this algorithm, use the following examples to add Model Monitor support to your product:\n",
    "For sample code to enable and monitor the model, see following notebooks:\n",
    "1. [Enable Amazon SageMaker Model Monitor](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_model_monitor/enable_model_monitor/SageMaker-Enable-Model-Monitor.ipynb)\n",
    "2. [Amazon SageMaker Model Monitor - visualizing monitoring results](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_model_monitor/visualization/SageMaker-Model-Monitor-Visualize.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. you can terminate the same to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is an experiment, you do not need to run a hyperparameter tuning job. However, if you would like to see how to tune a model trained using a third-party algorithm with Amazon SageMaker's hyperparameter tuning functionality, you can run the optional tuning step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform Batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will perform batch inference using multiple input payloads together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the batch-transform job input files to S3\n",
    "\n",
    "np.save(\"test.npy\", np.expand_dims(np.expand_dims(norm, 0), 2))\n",
    "\n",
    "transform_input_folder = \"test.npy\"\n",
    "transform_input = sagemaker_session.upload_data(transform_input_folder, key_prefix=model_name)\n",
    "print(\"Transform input uploaded to \" + transform_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the batch-transform job\n",
    "\n",
    "output_path_s3 = \"s3://{}/inference/output/\".format(bucket)\n",
    "\n",
    "transformer = estimator.transformer(\n",
    "                        instance_count= 1,\n",
    "                        instance_type=  batch_transform_inference_instance_type,\n",
    "                        strategy= \"SingleRecord\",\n",
    "                        output_path= output_path_s3,\n",
    "                        accept= \"application/x-npy\")\n",
    "\n",
    "transformer.transform(data=transform_input,\n",
    "                    content_type='application/x-npy',\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output is available on following path\n",
    "transformer.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the algorithm, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/aws_marketplace|curating_aws_marketplace_listing_and_sample_notebook|Algorithm|Sample_Notebook_Template|title_of_your_product-Algorithm.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
